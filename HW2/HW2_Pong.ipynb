{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "\n",
    "learning_rate = 0.01\n",
    "state_size = 6400\n",
    "n_actions = env.action_space.n #number of available actions\n",
    "\n",
    "#gamma = 0.99 # discount rate\n",
    "learning_rate = 0.01\n",
    "episodes = 100\n",
    "epochs = 100\n",
    "base_or_nah = True # baseline or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma = .99, normalize = False):\n",
    "    \"\"\"discount rewards and return a list\"\"\"\n",
    "    discount_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discount_r[t] = running_add      \n",
    "    \n",
    "    if normalize:\n",
    "        mean = np.mean(discount_r)\n",
    "        std = np.std(discount_r)\n",
    "        discount_r = (discount_r - mean) / (std)\n",
    "    \n",
    "    return list(discount_r)\n",
    "\n",
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return np.reshape(image.astype(np.float).ravel(), [80,80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## build model\n",
    "class modeling():\n",
    "    def __init__(self, learning_rate=0.01, state_size=state_size, action_size=n_actions, name=\"PolicyTuning\"):\n",
    "    #state, actions, discounted_episods_rewards_, learning_rate:\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, 80,80], name=\"states\")\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "            self.discounted_episodes_rewards_ = tf.placeholder(tf.float32, [None,], name=\"expected_episode_rewards\")\n",
    "        \n",
    "        with tf.name_scope(\"conv1\"):\n",
    "            self.conv1_out = tf.layers.conv1d(self.inputs_, filters=32, kernel_size = 8, strides=4,\n",
    "                                              padding = \"VALID\", \n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            #self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1, training = True, epsilon=learning_rate, name=\"batch_norm1\")\n",
    "            #self.conv1_out = tf.nn.relu(self.conv1_batchnorm, name=\"conv_out\")\n",
    "\n",
    "        with tf.name_scope(\"conv2\"):\n",
    "            self.conv2_out = tf.contrib.layers.fully_connected(self.conv1_out, num_outputs=64, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            #self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2, training = True, epsilon=learning_rate, name=\"batch_norm2\")\n",
    "            #self.conv2_out = tf.nn.relu(self.conv2_batchnorm, name=\"conv_out\")\n",
    "\n",
    "        with tf.name_scope(\"conv3\"):\n",
    "            self.conv3_out = tf.contrib.layers.fully_connected(self.conv2_out, num_outputs=32, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            #self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3, training = True, epsilon=learning_rate, name=\"batch_norm3\")\n",
    "            #self.conv3_out = tf.nn.relu(self.conv3_batchnorm, name=\"conv_out\")\n",
    "\n",
    "        with tf.name_scope(\"flatten\"):\n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "            self.logits = tf.layers.dense(inputs = self.flatten, units = n_actions, activation = tf.nn.relu, \n",
    "                                     kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "        with tf.name_scope(\"softmax\"):\n",
    "            self.action_distribution = tf.nn.softmax(self.logits)\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logits, \n",
    "                                                                            labels = self.actions_)\n",
    "            self.to_op = tf.multiply(self.cross_entropy, self.discounted_episodes_rewards_)\n",
    "            self.loss = tf.reduce_mean(self.to_op)\n",
    "            self.learn= tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "class Baseline():\n",
    "    def __init__(self, learning_rate=0.01, state_size=6400, name=\"Baseline\"):\n",
    "        with tf.variable_scope(name):\n",
    "\n",
    "            # Store Variables\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, 80, 80], name='inputs')\n",
    "            self.discounted_episodes_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episodes_rewards_\")\n",
    "\n",
    "            # Hidden Layers\n",
    "            #self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, 10, \n",
    "            #                                             weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc1 = tf.layers.conv1d(self.inputs_, filters=32, kernel_size = 8, strides=4,\n",
    "                                              padding = \"VALID\", \n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, 8, \n",
    "                                                         weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc3 = tf.contrib.layers.fully_connected(self.fc2, 1, activation_fn=None, \n",
    "                                                                 weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            # Define Loss\n",
    "            self.loss = tf.reduce_mean(tf.square(self.fc3 - self.discounted_episodes_rewards_), name=\"mse\")\n",
    "\n",
    "            # Adjust Network\n",
    "            self.learn = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "Pong = modeling(name='Pong', learning_rate=learning_rate, state_size=state_size, action_size=n_actions)\n",
    "baseline = Baseline(name = 'Base', learning_rate=learning_rate, state_size=state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "all_rewards = []\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        \n",
    "        # save all states, actions, and rewards that happen \n",
    "        all_states, all_actions, all_discount_rewards = [], [], []\n",
    "        running_rewards = []\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            \n",
    "            state = env.reset()\n",
    "            episode_rewards = []\n",
    "            \n",
    "            while True:            \n",
    "                # get action prob distribution\n",
    "                action_dist = sess.run(Pong.action_distribution, \n",
    "                                       feed_dict={Pong.inputs_: preprocess(state).reshape(1, 80, 80)})\n",
    "                \n",
    "                # select action per distribution\n",
    "                action = np.random.choice(range(action_dist.shape[1]), p=action_dist.ravel())\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                # record all observed values\n",
    "                all_states.append(preprocess(state).reshape(1, 80, 80))\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                # action 0 or 1\n",
    "                which_action = np.zeros(action_dist.shape[1])\n",
    "                which_action[action] = 1\n",
    "                all_actions.append(which_action)\n",
    "                \n",
    "                # reset current state to be new state\n",
    "                state = new_state\n",
    "                \n",
    "                if done:\n",
    "                    # Calculate discounted reward per episode\n",
    "                    dis_rewards = discount_rewards(episode_rewards)\n",
    "                    all_discount_rewards += dis_rewards\n",
    "                    \n",
    "                    # record reward per episode\n",
    "                    running_rewards.append(sum(episode_rewards))\n",
    "                    break\n",
    "        \n",
    "        if base_or_nah: # baseline adjustment\n",
    "            original_r = all_discount_rewards.copy()\n",
    "            baseline_r = sess.run(baseline.fc3, feed_dict={baseline.inputs_ : np.vstack(all_states)})\n",
    "            all_discount_rewards -= np.hstack(baseline_r)\n",
    "\n",
    "            # train on baseline\n",
    "            _, _= sess.run([baseline.loss, baseline.learn], \n",
    "                        feed_dict={baseline.inputs_: np.vstack(all_states),\n",
    "                        baseline.discounted_episodes_rewards_: original_r })\n",
    "         \n",
    "        # Policy Gradient Descent Update\n",
    "        _, _= sess.run([Pong.loss, Pong.learn], \n",
    "                                     feed_dict={Pong.inputs_: np.vstack(all_states),\n",
    "                                     Pong.actions_: np.vstack(all_actions),\n",
    "                                     Pong.discounted_episodes_rewards_: all_discount_rewards })      \n",
    "        \n",
    "        # average reward per episodes in epoch\n",
    "        all_rewards.append(np.mean(running_rewards))\n",
    "\n",
    "# rewards over training epochs\n",
    "plt.plot(all_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
